{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üì¶ Preprocessing and Feature Engineering in Python of the Energy and Weather datasets combineds\n",
    "\n",
    "In this notebook, we begin the modeling phase using the cleaned and merged dataset previously generated during the EDA stage in `R`. The data includes hourly electricity demand, energy production by source, pricing, and weather observations.\n",
    "\n",
    "## üì¶ 1. Preprocessing & Feature Engineering\n",
    "\n",
    "### üßº 1.1 Data Source & Structure\n",
    "\n",
    "We start by:\n",
    "\n",
    "1. Importing the cleaned dataset (`combined_clean.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# parse_dates=[\"datetime\"] to ensure convert the datetime column to datetime objects\n",
    "df = pd.read_csv(\"../data/processed/combined_clean.csv\", parse_dates=[\"datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "The dataset `combined_clean.csv` was created from a merge of two raw data sources:\n",
    "\n",
    "- **Energy data**: Hourly electricity consumption, generation, and pricing.\n",
    "- **Weather data**: Meteorological conditions across Spain‚Äôs five largest cities.\n",
    "\n",
    "The data was preprocessed in `R` and saved to disk for use in Python.\n",
    "\n",
    "#### üîé 1.1.1 Cleaning Decisions (Already Applied in R)\n",
    "\n",
    "These preprocessing steps were applied in the EDA notebook:\n",
    "\n",
    "- Removed two features with **100% missing values**:\n",
    "  - `generation_hydro_pumped_storage_aggregated`\n",
    "  - `forecast_wind_offshore_eday_ahead`\n",
    "- Renamed timestamp fields to `datetime` and ensured consistency.\n",
    "- Merged energy and weather data using a `left_join()` on datetime.\n",
    "- Extracted **time-based features**: `hour`, `wday` (weekday), and `month`.\n",
    "- Exported the cleaned dataset as `data/processed/combined_clean.csv` (from the root directory of the project)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "2. Verifying the structure and consistency of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "##### üîç Visual Inspection: Energy Load Over Time\n",
    "\n",
    "This plot gives an initial view of the total electricity demand over the entire time range. We expect to observe:\n",
    "\n",
    "- Daily and seasonal patterns\n",
    "- Sudden drops or spikes (potential anomalies or holidays)\n",
    "- Long-term trends in consumption\n",
    "\n",
    "This will help guide our feature engineering and baseline modeling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df[\"datetime\"], df[\"total_load_actual\"], alpha=0.7)\n",
    "plt.title(\"Energy Load Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Load (MW)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "3. Performing additional feature engineering if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è 1.2 Missing Data Status\n",
    "\n",
    "Although major missing columns were removed, **some NA values still remain** (as seen in `df.info()`):\n",
    "\n",
    "- These were not removed during EDA to preserve full context for visualization.\n",
    "- Now, before modeling, we should consider:\n",
    "  - üóëÔ∏è **Removing remaining NA rows**\n",
    "  - üß† Or **imputing missing values if relevant**\n",
    "\n",
    "üëâ You can inspect this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### ‚úÖ 1.3 Remove Remaining Missing Values\n",
    "\n",
    "Although only a tiny fraction of rows have missing values (less than 0.1%), we will remove them to ensure clean modeling pipelines.\n",
    "\n",
    "- The following operation **removes all rows** that contain at least one `NaN` value.\n",
    "- This preserves temporal consistency and avoids imputing potentially misleading values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with at least one missing value\n",
    "initial_shape = df.shape\n",
    "df.dropna(inplace=True)\n",
    "final_shape = df.shape\n",
    "\n",
    "print(f\"Removed {initial_shape[0] - final_shape[0]} rows with missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "üí° **Note**: With over 178,000 rows, this operation removes less than 0.1% of the data and simplifies the modeling workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### üì¶ 1.4 Encode Temporal Features\n",
    "\n",
    "We encode the weekday (wday) and month (month) columns as integers to prepare them for machine learning models.\n",
    "\n",
    "Since both features have a natural order (e.g., Monday to Sunday, January to December), we apply ordinal encoding instead of one-hot encoding for simplicity and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert weekday (Mon‚ÄìSun) and month (Jan‚ÄìDec) to numerical values\n",
    "# Ensure proper categorical order before conversion\n",
    "\n",
    "\n",
    "# Define ordered categories\n",
    "wday_order = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "month_order = [\n",
    "    \"Jan\",\n",
    "    \"Feb\",\n",
    "    \"Mar\",\n",
    "    \"Apr\",\n",
    "    \"May\",\n",
    "    \"Jun\",\n",
    "    \"Jul\",\n",
    "    \"Aug\",\n",
    "    \"Sep\",\n",
    "    \"Oct\",\n",
    "    \"Nov\",\n",
    "    \"Dec\",\n",
    "]\n",
    "\n",
    "# Set categorical types\n",
    "df[\"wday\"] = (\n",
    "    df[\"wday\"].astype(CategoricalDtype(categories=wday_order, ordered=True)).cat.codes\n",
    ")\n",
    "df[\"month\"] = (\n",
    "    df[\"month\"].astype(CategoricalDtype(categories=month_order, ordered=True)).cat.codes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "üí° This encoding transforms weekdays to integers from 0 (Mon) to 6 (Sun), and months from 0 (Jan) to 11 (Dec), preserving ordinal structure while simplifying downstream model input.\n",
    "\n",
    "##### üß† Why We Used `pandas.api.types.CategoricalDtype` instead of `LabelEncoder`\n",
    "\n",
    "Although scikit-learn provides tools like `LabelEncoder` to convert categorical text to numeric values, we chose to use `pandas.api.types.CategoricalDtype(...).cat.codes` for the following reasons:\n",
    "\n",
    "- ‚úÖ **Explicit order control**: `CategoricalDtype` allows us to manually specify the natural order of categories (e.g., Monday ‚Üí Sunday), which is essential for time-based features like `wday` and `month`.\n",
    "- ‚úÖ **Cleaner integration**: Since our data is already loaded in a `pandas` DataFrame, this approach avoids the need to fit an encoder or manage external objects.\n",
    "- ‚úÖ **NaN-safe**: `pandas` preserves missing values gracefully, whereas `LabelEncoder` throws errors on `NaN`s (even tough we have eliminated `NaN` values befor this step).\n",
    "- ‚úÖ **More transparent**: The mapping between categories and codes is easily inspectable and reversible using .categories.\n",
    "\n",
    "> In contrast, `LabelEncoder` is better suited for encoding target variables or when using fully automated pipelines with scikit-learn.\n",
    "\n",
    "For this reason, our encoding is both more interpretable and better aligned with temporal modeling needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### üîÅ 1.5 Lag and Rolling Features\n",
    "\n",
    "To capture historical behavior and temporal dependencies, we create lagged and rolling window features based on `total_load_actual`.\n",
    "\n",
    "- **Lag features** allow the model to ‚Äúlook back‚Äù at past values, which is crucial in time series forecasting.\n",
    "- **Rolling features** help smooth short-term fluctuations and highlight trends or cycles.\n",
    "\n",
    "These engineered features are often predictive in energy demand and similar applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sort the dataframe by city_name and datetime\n",
    "# to ensure that the lagged and rolling features are calculated correctly\n",
    "df = df.sort_values([\"city_name\", \"datetime\"])\n",
    "\n",
    "# Lagged features\n",
    "df[\"load_lag_1h\"] = df.groupby(\"city_name\")[\"total_load_actual\"].shift(1)\n",
    "df[\"load_lag_24h\"] = df.groupby(\"city_name\")[\"total_load_actual\"].shift(24)\n",
    "\n",
    "# Rolling mean features\n",
    "df[\"load_roll_24h\"] = (\n",
    "    df.groupby(\"city_name\")[\"total_load_actual\"]\n",
    "    .rolling(24)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")\n",
    "df[\"load_roll_7d\"] = (\n",
    "    df.groupby(\"city_name\")[\"total_load_actual\"]\n",
    "    .rolling(24 * 7)\n",
    "    .mean()\n",
    "    .reset_index(level=0, drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### üèôÔ∏è City-aware Lag and Rolling Features\n",
    "\n",
    "‚ö†Ô∏è **Important!** Since each timestamp contains one row per city, applying lag features globally would mix time dependencies across cities. This would create data leakage and distort true temporal signals.\n",
    "\n",
    "To avoid this, we compute lag and rolling statistics **separately for each city** as below\n",
    "\n",
    "These features are essential for capturing temporal patterns in energy demand at the city level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "As part of the feature engineering process at the last step, we obtained a lot of `NaN` values that we should handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### ‚ö†Ô∏è Handling NaN Values After Lag & Rolling Features\n",
    "\n",
    "The introduction of lagged and rolling features (e.g., `load_lag_1h`, `load_lag_24h`, `load_roll_24h`, `load_roll_7d`) naturally creates missing values at the start of each city‚Äôs time series. For example:\n",
    "\n",
    "- `load_lag_1h` is missing the first hour,\n",
    "- `load_lag_24h` and `load_roll_24h` require at least 24 hours of data,\n",
    "- `load_roll_7d` needs a full 7-day (168-hour) history to compute a valid rolling mean.\n",
    "\n",
    "Since our dataset contains hourly entries **per city**, these gaps occur **at the beginning of each city‚Äôs series**. In total, these missing values account for **835 rows**, or roughly **0.47% of the dataset**.\n",
    "\n",
    "üîé We choose to drop these rows rather than impute them because:\n",
    "\n",
    "- They occur at the very start of each time series where lag/rolling windows are not defined.\n",
    "- Imputing them would distort the statistical meaning of the features.\n",
    "- The proportion of data lost is negligible compared to the overall dataset size.\n",
    "\n",
    "üëâ Final step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any NaN values created by lag/rolling operations\n",
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "This ensures our dataset is clean and consistent before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### üîö 1.6 Summary: Ready for Modeling\n",
    "\n",
    "At this stage, we‚Äôve completed the preprocessing and engineered useful time-based features:\n",
    "\n",
    "- Removed noisy or fully missing columns.\n",
    "- Extracted temporal variables (`hour`, `weekday`, `month`) as encoded variables.\n",
    "- Generated lag features to capture short- and medium-term temporal dependencies.\n",
    "- Added rolling means for signal smoothing and trend detection.\n",
    "- Ensured all data is clean (dropna) and has a fresh index (reset_index).\n",
    "\n",
    "üí° We now have a fully prepared feature matrix for modeling energy consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We export the final DataFrame to a CSV file\n",
    "df.to_csv(\"../data/processed/final_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prediction-temporal-series-p2enJBmg-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
